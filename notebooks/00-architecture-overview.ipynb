{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 00: AI Dynamo Architecture Overview\n",
    "\n",
    "> **Systems Engineering Perspective**: Understanding Dynamo as a distributed systems problem\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this module, you will:\n",
    "\n",
    "1. **Map** Dynamo components to familiar distributed systems patterns\n",
    "2. **Understand** why LLM inference requires specialized infrastructure\n",
    "3. **Trace** the lifecycle of a request through the system\n",
    "4. **Identify** the role of each infrastructure component (etcd, NATS, NIXL)\n",
    "5. **Explain** disaggregated serving using systems terminology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites Check\n",
    "\n",
    "This module is conceptual - no Dynamo installation required yet. We just need basic Python to visualize concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.12.3 (main, Aug 14 2025, 17:47:21) [GCC 13.3.0]\n",
      "Platform: Linux-6.11.0-1016-nvidia-aarch64-with-glibc2.39\n",
      "Architecture: aarch64\n",
      "\n",
      "GPU(s) detected:\n",
      "  GPU 0: NVIDIA GB10, [N/A]\n"
     ]
    }
   ],
   "source": [
    "# Basic environment check\n",
    "import sys\n",
    "import platform\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Platform: {platform.platform()}\")\n",
    "print(f\"Architecture: {platform.machine()}\")\n",
    "\n",
    "# Check if we're on a GPU-enabled system (informational only)\n",
    "try:\n",
    "    import subprocess\n",
    "    result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader'], \n",
    "                          capture_output=True, text=True, timeout=5)\n",
    "    if result.returncode == 0:\n",
    "        print(f\"\\nGPU(s) detected:\")\n",
    "        for i, line in enumerate(result.stdout.strip().split('\\n')):\n",
    "            print(f\"  GPU {i}: {line}\")\n",
    "    else:\n",
    "        print(\"\\nNo GPU detected (that's OK for this module)\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nCould not check GPU: {e} (that's OK for this module)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: The Mental Model\n",
    "\n",
    "### From Web Services to LLM Inference\n",
    "\n",
    "If you've built distributed web services, you already understand 90% of Dynamo. Let's map the concepts:\n",
    "\n",
    "| **Distributed Systems Concept** | **Dynamo Equivalent** | **Why It Matters** |\n",
    "|--------------------------------|----------------------|-------------------|\n",
    "| Reverse proxy (Nginx) | Frontend | HTTP handling, API translation |\n",
    "| Load balancer with session affinity | Router | KV cache-aware request routing |\n",
    "| Backend service instances | Workers | GPU inference engines |\n",
    "| Service registry (Consul/ZK) | etcd | Dynamic worker discovery |\n",
    "| Message queue (Kafka/RabbitMQ) | NATS JetStream | KV cache events, metrics |\n",
    "| Session cache (Redis) | KV Cache | Attention state per request |\n",
    "| DMA / RDMA | NIXL | Zero-copy GPU-to-GPU transfer |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dynamo Component Reference\n",
      "================================================================================\n",
      "\n",
      "Frontend\n",
      "  Systems Analog : Nginx reverse proxy\n",
      "  Language       : Rust\n",
      "  Responsibility : HTTP handling, OpenAI API compatibility, request validation\n",
      "  Default Port   : 8000\n",
      "\n",
      "Router\n",
      "  Systems Analog : Load balancer with session affinity\n",
      "  Language       : Rust\n",
      "  Responsibility : Route requests to optimal worker based on KV cache hits\n",
      "  Default Port   : Internal\n",
      "\n",
      "Worker\n",
      "  Systems Analog : Backend service instance\n",
      "  Language       : Python (vLLM/SGLang/TRT-LLM)\n",
      "  Responsibility : GPU inference - model loading, prefill, decode\n",
      "  Default Port   : 8080\n",
      "\n",
      "etcd\n",
      "  Systems Analog : Consul / ZooKeeper\n",
      "  Language       : Go\n",
      "  Responsibility : Service discovery, worker registration, health tracking\n",
      "  Default Port   : 2379\n",
      "\n",
      "NATS\n",
      "  Systems Analog : Kafka / RabbitMQ\n",
      "  Language       : Go\n",
      "  Responsibility : Event streaming, KV cache notifications, metrics\n",
      "  Default Port   : 4222\n",
      "\n",
      "NIXL\n",
      "  Systems Analog : RDMA / DMA\n",
      "  Language       : C/Rust\n",
      "  Responsibility : Zero-copy KV cache transfer between GPUs\n",
      "  Default Port   : InfiniBand\n"
     ]
    }
   ],
   "source": [
    "# Let's visualize this mapping\n",
    "component_mapping = {\n",
    "    \"Frontend\": {\n",
    "        \"systems_analog\": \"Nginx reverse proxy\",\n",
    "        \"language\": \"Rust\",\n",
    "        \"responsibility\": \"HTTP handling, OpenAI API compatibility, request validation\",\n",
    "        \"port\": 8000\n",
    "    },\n",
    "    \"Router\": {\n",
    "        \"systems_analog\": \"Load balancer with session affinity\",\n",
    "        \"language\": \"Rust\",\n",
    "        \"responsibility\": \"Route requests to optimal worker based on KV cache hits\",\n",
    "        \"port\": \"Internal\"\n",
    "    },\n",
    "    \"Worker\": {\n",
    "        \"systems_analog\": \"Backend service instance\",\n",
    "        \"language\": \"Python (vLLM/SGLang/TRT-LLM)\",\n",
    "        \"responsibility\": \"GPU inference - model loading, prefill, decode\",\n",
    "        \"port\": 8080\n",
    "    },\n",
    "    \"etcd\": {\n",
    "        \"systems_analog\": \"Consul / ZooKeeper\",\n",
    "        \"language\": \"Go\",\n",
    "        \"responsibility\": \"Service discovery, worker registration, health tracking\",\n",
    "        \"port\": 2379\n",
    "    },\n",
    "    \"NATS\": {\n",
    "        \"systems_analog\": \"Kafka / RabbitMQ\",\n",
    "        \"language\": \"Go\",\n",
    "        \"responsibility\": \"Event streaming, KV cache notifications, metrics\",\n",
    "        \"port\": 4222\n",
    "    },\n",
    "    \"NIXL\": {\n",
    "        \"systems_analog\": \"RDMA / DMA\",\n",
    "        \"language\": \"C/Rust\",\n",
    "        \"responsibility\": \"Zero-copy KV cache transfer between GPUs\",\n",
    "        \"port\": \"InfiniBand\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Dynamo Component Reference\")\n",
    "print(\"=\" * 80)\n",
    "for component, details in component_mapping.items():\n",
    "    print(f\"\\n{component}\")\n",
    "    print(f\"  Systems Analog : {details['systems_analog']}\")\n",
    "    print(f\"  Language       : {details['language']}\")\n",
    "    print(f\"  Responsibility : {details['responsibility']}\")\n",
    "    print(f\"  Default Port   : {details['port']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Why LLM Inference is Different\n",
    "\n",
    "### The Two-Phase Nature of LLM Requests\n",
    "\n",
    "Unlike traditional web requests (receive â†’ process â†’ respond), LLM inference has **two distinct phases** with completely different resource profiles:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                        LLM Request Lifecycle                            â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                         â”‚\n",
    "â”‚   User Prompt â”€â”€â”€â–¶ [PREFILL PHASE] â”€â”€â”€â–¶ [DECODE PHASE] â”€â”€â”€â–¶ Response   â”‚\n",
    "â”‚                                                                         â”‚\n",
    "â”‚   \"Explain           Process entire       Generate tokens               â”‚\n",
    "â”‚    quantum           prompt at once       one-by-one                    â”‚\n",
    "â”‚    physics\"          (compute KV cache)   (streaming)                   â”‚\n",
    "â”‚                                                                         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's understand the resource profiles of each phase\n",
    "\n",
    "phase_characteristics = {\n",
    "    \"Prefill Phase\": {\n",
    "        \"description\": \"Process entire input prompt, compute attention for all tokens\",\n",
    "        \"compute_pattern\": \"Compute-bound (matrix multiplications)\",\n",
    "        \"gpu_utilization\": \"HIGH (80-95%)\",\n",
    "        \"memory_access\": \"Sequential, cache-friendly\",\n",
    "        \"batching\": \"Highly parallelizable across prompts\",\n",
    "        \"latency_metric\": \"Time-to-First-Token (TTFT)\",\n",
    "        \"systems_analog\": \"Batch processing job (like video transcoding)\"\n",
    "    },\n",
    "    \"Decode Phase\": {\n",
    "        \"description\": \"Generate output tokens one at a time, autoregressively\",\n",
    "        \"compute_pattern\": \"Memory-bandwidth bound (reading KV cache)\",\n",
    "        \"gpu_utilization\": \"LOW per token (20-40%)\",\n",
    "        \"memory_access\": \"Random access to KV cache\",\n",
    "        \"batching\": \"Limited by memory, not compute\",\n",
    "        \"latency_metric\": \"Inter-Token Latency (ITL)\",\n",
    "        \"systems_analog\": \"Real-time streaming (like live video)\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"LLM Inference Phase Comparison\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for phase, chars in phase_characteristics.items():\n",
    "    print(f\"\\n{phase}\")\n",
    "    print(\"-\" * 40)\n",
    "    for key, value in chars.items():\n",
    "        print(f\"  {key:20}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Key Insight: Disaggregation\n",
    "\n",
    "Running prefill and decode on the same GPU is like running your:\n",
    "- **Batch ETL jobs** (CPU-intensive, high throughput)\n",
    "- **Real-time API servers** (latency-sensitive, consistent response time)\n",
    "\n",
    "...on the same machine. It works, but it's inefficient.\n",
    "\n",
    "**Dynamo's solution**: Separate prefill workers from decode workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing monolithic vs disaggregated serving\n",
    "\n",
    "monolithic_timeline = \"\"\"\n",
    "MONOLITHIC SERVING (Traditional)\n",
    "================================\n",
    "\n",
    "GPU 1: [====PREFILL====][==DECODE==][==DECODE==][==DECODE==]....\n",
    "                        â†‘\n",
    "                        TTFT (Time to First Token)\n",
    "\n",
    "GPU 2: [====PREFILL====][==DECODE==][==DECODE==][==DECODE==]....\n",
    "\n",
    "Problem: \n",
    "- During PREFILL, GPU is compute-saturated (good!)\n",
    "- During DECODE, GPU is memory-bandwidth limited (underutilized!)\n",
    "- New requests must wait for current decode to finish\n",
    "\"\"\"\n",
    "\n",
    "disaggregated_timeline = \"\"\"\n",
    "DISAGGREGATED SERVING (Dynamo)\n",
    "==============================\n",
    "\n",
    "Prefill GPU 1: [==PREFILL==][==PREFILL==][==PREFILL==][==PREFILL==]...\n",
    "                     â”‚            â”‚            â”‚\n",
    "                     â”‚ KV Cache   â”‚ KV Cache   â”‚ KV Cache\n",
    "                     â–¼ (via NIXL) â–¼            â–¼\n",
    "Decode GPU 2:  [=D=][=D=][=D=][=D=][=D=][=D=][=D=][=D=][=D=]...\n",
    "Decode GPU 3:  [=D=][=D=][=D=][=D=][=D=][=D=][=D=][=D=][=D=]...\n",
    "\n",
    "Benefits:\n",
    "- Prefill GPU stays compute-saturated (optimal for throughput)\n",
    "- Decode GPUs batch many concurrent decodes (better memory utilization)\n",
    "- Independent scaling of prefill vs decode capacity\n",
    "\"\"\"\n",
    "\n",
    "print(monolithic_timeline)\n",
    "print(disaggregated_timeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: The KV Cache - Your New Best Friend\n",
    "\n",
    "### What is the KV Cache?\n",
    "\n",
    "In transformer models, the **attention mechanism** computes relationships between all tokens. For each layer, it produces:\n",
    "- **K (Key)**: What information this token provides\n",
    "- **V (Value)**: The actual information content\n",
    "\n",
    "**The KV cache stores these computed attention states** so they don't need to be recomputed.\n",
    "\n",
    "### Systems Analogy: Memoization / Session Cache\n",
    "\n",
    "Think of it like:\n",
    "- **Web sessions**: User logs in once, session token avoids re-authentication\n",
    "- **Function memoization**: Expensive computation cached by input\n",
    "- **Database query cache**: Same query returns cached results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculate KV cache sizes for common models\n",
    "\n",
    "def calculate_kv_cache_size(\n",
    "    num_layers: int,\n",
    "    num_kv_heads: int,\n",
    "    head_dim: int,\n",
    "    sequence_length: int,\n",
    "    dtype_bytes: int = 2  # FP16 = 2 bytes, FP8 = 1 byte\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Calculate KV cache memory requirements.\n",
    "    \n",
    "    KV Cache size = 2 * num_layers * num_kv_heads * head_dim * seq_len * dtype_bytes\n",
    "    (2 for K and V)\n",
    "    \"\"\"\n",
    "    bytes_per_token = 2 * num_layers * num_kv_heads * head_dim * dtype_bytes\n",
    "    total_bytes = bytes_per_token * sequence_length\n",
    "    \n",
    "    return {\n",
    "        \"bytes_per_token\": bytes_per_token,\n",
    "        \"mb_per_token\": bytes_per_token / (1024 * 1024),\n",
    "        \"total_bytes\": total_bytes,\n",
    "        \"total_mb\": total_bytes / (1024 * 1024),\n",
    "        \"total_gb\": total_bytes / (1024 * 1024 * 1024)\n",
    "    }\n",
    "\n",
    "# Common model configurations\n",
    "models = {\n",
    "    \"TinyLlama-1.1B\": {\"num_layers\": 22, \"num_kv_heads\": 4, \"head_dim\": 64},\n",
    "    \"Llama-3-8B\": {\"num_layers\": 32, \"num_kv_heads\": 8, \"head_dim\": 128},\n",
    "    \"Llama-3-70B\": {\"num_layers\": 80, \"num_kv_heads\": 8, \"head_dim\": 128},\n",
    "    \"DeepSeek-R1-671B\": {\"num_layers\": 61, \"num_kv_heads\": 128, \"head_dim\": 128},  # MoE\n",
    "}\n",
    "\n",
    "# Calculate for different sequence lengths\n",
    "seq_lengths = [512, 2048, 8192, 32768, 131072]  # Up to 128K context\n",
    "\n",
    "print(\"KV Cache Memory Requirements (FP16)\")\n",
    "print(\"=\" * 90)\n",
    "print(f\"{'Model':<20} {'Seq Len':>10} {'Per Token':>12} {'Total Cache':>15} {'% of H100 80GB':>15}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for model_name, config in models.items():\n",
    "    for seq_len in seq_lengths:\n",
    "        result = calculate_kv_cache_size(\n",
    "            num_layers=config[\"num_layers\"],\n",
    "            num_kv_heads=config[\"num_kv_heads\"],\n",
    "            head_dim=config[\"head_dim\"],\n",
    "            sequence_length=seq_len\n",
    "        )\n",
    "        h100_pct = (result[\"total_gb\"] / 80) * 100\n",
    "        \n",
    "        if seq_len == 512:  # Only print model name once\n",
    "            print(f\"{model_name:<20} {seq_len:>10} {result['mb_per_token']:>10.3f} MB {result['total_gb']:>12.2f} GB {h100_pct:>13.1f}%\")\n",
    "        else:\n",
    "            print(f\"{'':<20} {seq_len:>10} {result['mb_per_token']:>10.3f} MB {result['total_gb']:>12.2f} GB {h100_pct:>13.1f}%\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why KV Cache Routing Matters\n",
    "\n",
    "Look at those numbers! For a 70B model with 32K context, the KV cache is **20+ GB**.\n",
    "\n",
    "Now imagine a multi-turn conversation:\n",
    "\n",
    "```\n",
    "Turn 1: \"Explain quantum physics\"           â†’ Compute 500 tokens of KV cache\n",
    "Turn 2: \"Now explain entanglement\"          â†’ Reuse 500 tokens, add 100 more\n",
    "Turn 3: \"What about quantum computing?\"     â†’ Reuse 600 tokens, add 100 more\n",
    "```\n",
    "\n",
    "**Without KV-aware routing**: Each turn might go to a different worker, recomputing from scratch.\n",
    "\n",
    "**With KV-aware routing**: Dynamo routes to the worker that already has the prefix cached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulating the impact of KV cache routing\n",
    "\n",
    "import random\n",
    "\n",
    "def simulate_routing(\n",
    "    num_requests: int,\n",
    "    num_workers: int,\n",
    "    conversation_turns: int,\n",
    "    tokens_per_turn: int,\n",
    "    compute_time_per_token_ms: float,  # Time to compute KV cache for one token\n",
    "    routing_strategy: str  # \"random\" or \"kv_aware\"\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Simulate routing efficiency for multi-turn conversations.\n",
    "    \"\"\"\n",
    "    total_compute_time = 0\n",
    "    total_cache_hits = 0\n",
    "    total_cache_misses = 0\n",
    "    \n",
    "    # Track which worker has which conversation's cache\n",
    "    conversation_worker = {}  # conversation_id -> worker_id\n",
    "    worker_cache = {i: set() for i in range(num_workers)}  # worker_id -> set of conversation_ids\n",
    "    \n",
    "    for conv_id in range(num_requests):\n",
    "        for turn in range(conversation_turns):\n",
    "            prefix_tokens = turn * tokens_per_turn  # Tokens from previous turns\n",
    "            new_tokens = tokens_per_turn\n",
    "            \n",
    "            if routing_strategy == \"random\":\n",
    "                worker = random.randint(0, num_workers - 1)\n",
    "            else:  # kv_aware\n",
    "                # Route to worker that has this conversation's cache\n",
    "                worker = conversation_worker.get(conv_id, random.randint(0, num_workers - 1))\n",
    "            \n",
    "            # Check cache hit\n",
    "            if conv_id in worker_cache[worker]:\n",
    "                # Cache hit - only compute new tokens\n",
    "                compute_time = new_tokens * compute_time_per_token_ms\n",
    "                total_cache_hits += 1\n",
    "            else:\n",
    "                # Cache miss - compute all tokens (prefix + new)\n",
    "                compute_time = (prefix_tokens + new_tokens) * compute_time_per_token_ms\n",
    "                total_cache_misses += 1\n",
    "                # Update cache\n",
    "                worker_cache[worker].add(conv_id)\n",
    "            \n",
    "            total_compute_time += compute_time\n",
    "            conversation_worker[conv_id] = worker\n",
    "    \n",
    "    total_turns = num_requests * conversation_turns\n",
    "    return {\n",
    "        \"total_compute_time_ms\": total_compute_time,\n",
    "        \"avg_latency_ms\": total_compute_time / total_turns,\n",
    "        \"cache_hit_rate\": total_cache_hits / total_turns * 100,\n",
    "        \"cache_misses\": total_cache_misses,\n",
    "        \"cache_hits\": total_cache_hits\n",
    "    }\n",
    "\n",
    "# Run simulation\n",
    "params = {\n",
    "    \"num_requests\": 100,          # 100 conversations\n",
    "    \"num_workers\": 4,             # 4 worker GPUs\n",
    "    \"conversation_turns\": 5,      # 5 turns per conversation\n",
    "    \"tokens_per_turn\": 500,       # 500 tokens per turn\n",
    "    \"compute_time_per_token_ms\": 0.5  # 0.5ms per token prefill\n",
    "}\n",
    "\n",
    "print(\"KV Cache Routing Simulation\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Scenario: {params['num_requests']} conversations, {params['conversation_turns']} turns each\")\n",
    "print(f\"Workers: {params['num_workers']}, Tokens/turn: {params['tokens_per_turn']}\")\n",
    "print()\n",
    "\n",
    "random_result = simulate_routing(**params, routing_strategy=\"random\")\n",
    "kv_aware_result = simulate_routing(**params, routing_strategy=\"kv_aware\")\n",
    "\n",
    "print(f\"{'Metric':<25} {'Random Routing':>20} {'KV-Aware Routing':>20}\")\n",
    "print(\"-\" * 65)\n",
    "print(f\"{'Total Compute Time':<25} {random_result['total_compute_time_ms']:>17.0f} ms {kv_aware_result['total_compute_time_ms']:>17.0f} ms\")\n",
    "print(f\"{'Avg Latency per Turn':<25} {random_result['avg_latency_ms']:>17.1f} ms {kv_aware_result['avg_latency_ms']:>17.1f} ms\")\n",
    "print(f\"{'Cache Hit Rate':<25} {random_result['cache_hit_rate']:>17.1f}% {kv_aware_result['cache_hit_rate']:>17.1f}%\")\n",
    "print(f\"{'Cache Misses':<25} {random_result['cache_misses']:>20} {kv_aware_result['cache_misses']:>20}\")\n",
    "print()\n",
    "\n",
    "speedup = random_result['total_compute_time_ms'] / kv_aware_result['total_compute_time_ms']\n",
    "print(f\"Speedup with KV-aware routing: {speedup:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Infrastructure Components\n",
    "\n",
    "### The Control Plane: etcd + NATS\n",
    "\n",
    "Dynamo uses two complementary systems for coordination:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                        CONTROL PLANE                                    â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                         â”‚\n",
    "â”‚  etcd (Service Discovery)              NATS JetStream (Events)          â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚\n",
    "â”‚  â”‚ /dynamo/workers/        â”‚          â”‚ dynamo.kv.events        â”‚       â”‚\n",
    "â”‚  â”‚   worker-1: {endpoint,  â”‚          â”‚   â€¢ cache.hit           â”‚       â”‚\n",
    "â”‚  â”‚              gpu,       â”‚          â”‚   â€¢ cache.miss          â”‚       â”‚\n",
    "â”‚  â”‚              status}    â”‚          â”‚   â€¢ cache.evict         â”‚       â”‚\n",
    "â”‚  â”‚   worker-2: {...}       â”‚          â”‚                         â”‚       â”‚\n",
    "â”‚  â”‚                         â”‚          â”‚ dynamo.planner.metrics  â”‚       â”‚\n",
    "â”‚  â”‚ Watch-based updates     â”‚          â”‚   â€¢ queue_depth         â”‚       â”‚\n",
    "â”‚  â”‚ Lease-based TTL         â”‚          â”‚   â€¢ ttft_p95            â”‚       â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚   â€¢ gpu_utilization     â”‚       â”‚\n",
    "â”‚                                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚\n",
    "â”‚                                                                         â”‚\n",
    "â”‚  USE CASE: \"Who exists?\"              USE CASE: \"What's happening?\"     â”‚\n",
    "â”‚  - Worker registration                - Real-time KV cache events        â”‚\n",
    "â”‚  - Health checks                      - Metrics streaming                â”‚\n",
    "â”‚  - Configuration                      - Planner decision triggers        â”‚\n",
    "â”‚                                                                         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: What worker registration looks like in etcd\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# This is what a worker would register in etcd\n",
    "worker_registration = {\n",
    "    \"worker_id\": \"worker-dgxspark01-gpu0\",\n",
    "    \"endpoint\": \"10.0.0.1:8080\",\n",
    "    \"grpc_endpoint\": \"10.0.0.1:8081\",\n",
    "    \"gpu\": {\n",
    "        \"name\": \"GH200\",\n",
    "        \"memory_gb\": 96,\n",
    "        \"compute_capability\": \"9.0\"\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"name\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "        \"quantization\": \"FP16\",\n",
    "        \"max_seq_len\": 8192\n",
    "    },\n",
    "    \"role\": \"prefill\",  # or \"decode\" or \"unified\"\n",
    "    \"status\": \"healthy\",\n",
    "    \"current_load\": {\n",
    "        \"active_requests\": 3,\n",
    "        \"queue_depth\": 7,\n",
    "        \"gpu_memory_used_gb\": 45.2\n",
    "    },\n",
    "    \"registered_at\": datetime.now().isoformat(),\n",
    "    \"lease_ttl_seconds\": 30\n",
    "}\n",
    "\n",
    "print(\"Example: Worker Registration in etcd\")\n",
    "print(\"Key: /dynamo/workers/worker-dgxspark01-gpu0\")\n",
    "print(\"Value:\")\n",
    "print(json.dumps(worker_registration, indent=2))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\\nExample etcdctl commands:\")\n",
    "print(\"-\" * 60)\n",
    "print(\"# Register a worker (with 30-second lease)\")\n",
    "print('etcdctl lease grant 30')\n",
    "print('etcdctl put /dynamo/workers/worker-1 \\'{\"endpoint\": \"...\"}\\' --lease=<LEASE_ID>')\n",
    "print()\n",
    "print(\"# List all workers\")\n",
    "print('etcdctl get --prefix /dynamo/workers/')\n",
    "print()\n",
    "print(\"# Watch for changes (Frontend uses this)\")\n",
    "print('etcdctl watch --prefix /dynamo/workers/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: What KV cache events look like in NATS\n",
    "\n",
    "kv_cache_events = [\n",
    "    {\n",
    "        \"subject\": \"dynamo.kv.cached\",\n",
    "        \"data\": {\n",
    "            \"worker_id\": \"worker-1\",\n",
    "            \"prefix_hash\": \"a3f2b8c1d4e5\",\n",
    "            \"prefix_tokens\": 512,\n",
    "            \"blocks_allocated\": 64,\n",
    "            \"memory_mb\": 256,\n",
    "            \"timestamp\": \"2026-02-01T10:30:00Z\"\n",
    "        },\n",
    "        \"description\": \"Worker cached a new prefix - Router updates its radix tree\"\n",
    "    },\n",
    "    {\n",
    "        \"subject\": \"dynamo.kv.hit\",\n",
    "        \"data\": {\n",
    "            \"worker_id\": \"worker-1\",\n",
    "            \"prefix_hash\": \"a3f2b8c1d4e5\",\n",
    "            \"hit_tokens\": 512,\n",
    "            \"request_id\": \"req-12345\",\n",
    "            \"saved_compute_ms\": 256\n",
    "        },\n",
    "        \"description\": \"Cache hit - request avoided recomputing 512 tokens\"\n",
    "    },\n",
    "    {\n",
    "        \"subject\": \"dynamo.kv.evicted\",\n",
    "        \"data\": {\n",
    "            \"worker_id\": \"worker-1\",\n",
    "            \"prefix_hash\": \"b4c3d2e1f0a9\",\n",
    "            \"reason\": \"memory_pressure\",\n",
    "            \"freed_mb\": 512\n",
    "        },\n",
    "        \"description\": \"Cache eviction - Router removes from radix tree\"\n",
    "    },\n",
    "    {\n",
    "        \"subject\": \"dynamo.planner.metrics\",\n",
    "        \"data\": {\n",
    "            \"worker_id\": \"worker-1\",\n",
    "            \"role\": \"prefill\",\n",
    "            \"queue_depth\": 15,\n",
    "            \"ttft_p50_ms\": 120,\n",
    "            \"ttft_p95_ms\": 450,\n",
    "            \"gpu_utilization\": 0.92\n",
    "        },\n",
    "        \"description\": \"Planner monitors this to decide scaling\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Example: KV Cache Events in NATS JetStream\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for event in kv_cache_events:\n",
    "    print(f\"\\nSubject: {event['subject']}\")\n",
    "    print(f\"Purpose: {event['description']}\")\n",
    "    print(f\"Data: {json.dumps(event['data'], indent=2)}\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data Plane: NIXL (RDMA for KV Cache)\n",
    "\n",
    "When prefill completes, the KV cache needs to move to the decode worker. This is where NIXL shines.\n",
    "\n",
    "```\n",
    "TRADITIONAL TCP TRANSFER:\n",
    "GPU A â†’ PCIe â†’ CPU â†’ TCP Stack â†’ NIC â†’ Network â†’ NIC â†’ TCP â†’ CPU â†’ PCIe â†’ GPU B\n",
    "        â†‘           â†‘                              â†‘           â†‘\n",
    "        Copy        Kernel                         Kernel      Copy\n",
    "        \n",
    "Latency: ~100-500Î¼s for small transfers, scales poorly\n",
    "CPU overhead: High (copies, syscalls)\n",
    "\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "NIXL WITH RDMA (GPU-Direct):\n",
    "GPU A â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ InfiniBand â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ GPU B\n",
    "                    (zero-copy, kernel bypass)\n",
    "                    \n",
    "Latency: ~1-5Î¼s\n",
    "CPU overhead: Near zero\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing transfer times: TCP vs RDMA\n",
    "\n",
    "def estimate_transfer_time(\n",
    "    data_size_gb: float,\n",
    "    method: str\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Estimate KV cache transfer time.\n",
    "    \"\"\"\n",
    "    if method == \"tcp_100gbe\":\n",
    "        # 100GbE = ~12.5 GB/s theoretical, ~10 GB/s practical\n",
    "        bandwidth_gbps = 10\n",
    "        latency_overhead_ms = 0.5  # TCP/IP stack overhead\n",
    "        cpu_overhead = \"High (multiple copies)\"\n",
    "    elif method == \"rdma_infiniband_200\":\n",
    "        # 200Gb InfiniBand = ~25 GB/s, near-wire speed with RDMA\n",
    "        bandwidth_gbps = 24\n",
    "        latency_overhead_ms = 0.005  # ~5Î¼s\n",
    "        cpu_overhead = \"Near zero\"\n",
    "    elif method == \"rdma_infiniband_400\":\n",
    "        # 400Gb InfiniBand (NDR) = ~50 GB/s\n",
    "        bandwidth_gbps = 48\n",
    "        latency_overhead_ms = 0.003\n",
    "        cpu_overhead = \"Near zero\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "    \n",
    "    transfer_time_ms = (data_size_gb / bandwidth_gbps) * 1000 + latency_overhead_ms\n",
    "    \n",
    "    return {\n",
    "        \"method\": method,\n",
    "        \"bandwidth_gbps\": bandwidth_gbps,\n",
    "        \"transfer_time_ms\": transfer_time_ms,\n",
    "        \"cpu_overhead\": cpu_overhead\n",
    "    }\n",
    "\n",
    "# KV cache sizes for different scenarios\n",
    "kv_cache_sizes = [\n",
    "    (\"Llama-8B, 2K context\", 0.5),\n",
    "    (\"Llama-8B, 8K context\", 2.0),\n",
    "    (\"Llama-70B, 4K context\", 10.0),\n",
    "    (\"Llama-70B, 32K context\", 80.0),\n",
    "]\n",
    "\n",
    "methods = [\"tcp_100gbe\", \"rdma_infiniband_200\", \"rdma_infiniband_400\"]\n",
    "\n",
    "print(\"KV Cache Transfer Time Comparison\")\n",
    "print(\"=\" * 90)\n",
    "print(f\"{'Scenario':<30} {'Size':>8} {'TCP 100GbE':>15} {'IB 200Gb':>15} {'IB 400Gb':>15}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for scenario, size_gb in kv_cache_sizes:\n",
    "    times = []\n",
    "    for method in methods:\n",
    "        result = estimate_transfer_time(size_gb, method)\n",
    "        times.append(result['transfer_time_ms'])\n",
    "    \n",
    "    print(f\"{scenario:<30} {size_gb:>6.1f}GB {times[0]:>12.1f}ms {times[1]:>12.1f}ms {times[2]:>12.1f}ms\")\n",
    "\n",
    "print(\"\\nKey Insight: For large KV caches (70B+ models), RDMA is essential for disaggregated serving.\")\n",
    "print(\"Without it, transfer time dominates TTFT.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Request Flow - Putting It All Together\n",
    "\n",
    "Let's trace a complete request through the Dynamo system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete request flow visualization\n",
    "\n",
    "request_flow = \"\"\"\n",
    "REQUEST FLOW THROUGH DYNAMO (Disaggregated Mode)\n",
    "================================================\n",
    "\n",
    "1. CLIENT REQUEST\n",
    "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "   â”‚ curl -X POST http://frontend:8000/v1/chat/completions \\             â”‚\n",
    "   â”‚   -d '{\"model\": \"llama-3-8b\", \"messages\": [{...}]}'                â”‚\n",
    "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                      â”‚\n",
    "                                      â–¼\n",
    "2. FRONTEND (Rust HTTP Server - Port 8000)\n",
    "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "   â”‚ â€¢ Validate OpenAI-compatible request                                â”‚\n",
    "   â”‚ â€¢ Extract prompt, compute prefix hash                               â”‚\n",
    "   â”‚ â€¢ Query Router: \"Which worker for prefix hash abc123?\"              â”‚\n",
    "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                      â”‚\n",
    "                                      â–¼\n",
    "3. ROUTER (Rust, embedded in Frontend)\n",
    "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "   â”‚ â€¢ Lookup prefix hash in global radix tree                           â”‚\n",
    "   â”‚   - Query etcd: healthy workers                                     â”‚\n",
    "   â”‚   - Query KV Block Manager: cache locations                         â”‚\n",
    "   â”‚ â€¢ Decision: Prefill-Worker-2 has 78% cache hit, queue depth 3       â”‚\n",
    "   â”‚ â€¢ Fallback: If no cache hit, pick least loaded prefill worker       â”‚\n",
    "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                      â”‚\n",
    "                                      â–¼\n",
    "4. PREFILL WORKER (Python + vLLM/SGLang)\n",
    "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "   â”‚ â€¢ Receive request via gRPC                                          â”‚\n",
    "   â”‚ â€¢ Check local KV cache for prefix                                   â”‚\n",
    "   â”‚   - HIT: Reuse cached KV blocks                                     â”‚\n",
    "   â”‚   - MISS: Compute attention for all input tokens                    â”‚\n",
    "   â”‚ â€¢ Publish to NATS: \"dynamo.kv.cached\" with block locations          â”‚\n",
    "   â”‚ â€¢ Initiate NIXL transfer to Decode Worker                           â”‚\n",
    "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                      â”‚\n",
    "                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                          â”‚    NIXL Transfer      â”‚\n",
    "                          â”‚  (RDMA, ~5Î¼s/block)   â”‚\n",
    "                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                      â”‚\n",
    "                                      â–¼\n",
    "5. DECODE WORKER (Python + vLLM/SGLang)\n",
    "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "   â”‚ â€¢ Receive KV cache blocks in GPU memory                             â”‚\n",
    "   â”‚ â€¢ Begin autoregressive token generation                             â”‚\n",
    "   â”‚ â€¢ Stream tokens back through Frontend                               â”‚\n",
    "   â”‚   - Token 1: \"The\" â†’ 10ms                                          â”‚\n",
    "   â”‚   - Token 2: \"answer\" â†’ 10ms                                        â”‚\n",
    "   â”‚   - Token 3: \"is\" â†’ 10ms                                           â”‚\n",
    "   â”‚   - ...                                                             â”‚\n",
    "   â”‚ â€¢ On completion, publish metrics to NATS                            â”‚\n",
    "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                      â”‚\n",
    "                                      â–¼\n",
    "6. RESPONSE TO CLIENT\n",
    "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "   â”‚ HTTP/1.1 200 OK                                                     â”‚\n",
    "   â”‚ Content-Type: text/event-stream                                     â”‚\n",
    "   â”‚                                                                     â”‚\n",
    "   â”‚ data: {\"choices\": [{\"delta\": {\"content\": \"The\"}}]}                 â”‚\n",
    "   â”‚ data: {\"choices\": [{\"delta\": {\"content\": \" answer\"}}]}             â”‚\n",
    "   â”‚ data: {\"choices\": [{\"delta\": {\"content\": \" is\"}}]}                 â”‚\n",
    "   â”‚ ...                                                                 â”‚\n",
    "   â”‚ data: [DONE]                                                        â”‚\n",
    "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "BACKGROUND: PLANNER\n",
    "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "   â”‚ â€¢ Subscribe to NATS: dynamo.planner.metrics                         â”‚\n",
    "   â”‚ â€¢ Monitor: queue depths, TTFT, ITL, GPU utilization                 â”‚\n",
    "   â”‚ â€¢ Detect: Long prompts increasing â†’ prefill queue growing           â”‚\n",
    "   â”‚ â€¢ Action: Scale prefill workers 2 â†’ 4 (zero-downtime)               â”‚\n",
    "   â”‚ â€¢ Update: etcd with new worker registrations                        â”‚\n",
    "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\"\"\"\n",
    "\n",
    "print(request_flow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Troubleshooting Guide\n",
    "\n",
    "When things go wrong (and they will), check in this order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Troubleshooting reference\n",
    "\n",
    "troubleshooting_guide = {\n",
    "    \"Worker not found\": {\n",
    "        \"likely_cause\": \"etcd registration failed or lease expired\",\n",
    "        \"debug_commands\": [\n",
    "            \"etcdctl get --prefix /dynamo/workers/\",\n",
    "            \"etcdctl lease list\",\n",
    "            \"curl http://frontend:8000/health\"\n",
    "        ],\n",
    "        \"fix\": \"Check worker logs for registration errors, verify etcd connectivity\"\n",
    "    },\n",
    "    \"Slow TTFT (Time-to-First-Token)\": {\n",
    "        \"likely_cause\": \"KV cache miss - request routed to worker without cached prefix\",\n",
    "        \"debug_commands\": [\n",
    "            \"nats sub 'dynamo.kv.*'\",\n",
    "            \"Check router logs for routing decisions\",\n",
    "            \"nvidia-smi dmon -s u  # GPU utilization\"\n",
    "        ],\n",
    "        \"fix\": \"Verify KV-aware routing is enabled, check radix tree updates\"\n",
    "    },\n",
    "    \"Connection refused\": {\n",
    "        \"likely_cause\": \"Service not started or wrong port\",\n",
    "        \"debug_commands\": [\n",
    "            \"ss -tlnp | grep 8000  # Frontend\",\n",
    "            \"ss -tlnp | grep 2379  # etcd\",\n",
    "            \"ss -tlnp | grep 4222  # NATS\"\n",
    "        ],\n",
    "        \"fix\": \"Start the service, check firewall rules\"\n",
    "    },\n",
    "    \"CUDA Out of Memory\": {\n",
    "        \"likely_cause\": \"KV cache exhausted GPU memory\",\n",
    "        \"debug_commands\": [\n",
    "            \"nvidia-smi\",\n",
    "            \"Check max_num_seqs and max_model_len settings\",\n",
    "            \"nats sub 'dynamo.kv.evicted'  # Watch for evictions\"\n",
    "        ],\n",
    "        \"fix\": \"Reduce batch size, enable KV cache offloading to CPU\"\n",
    "    },\n",
    "    \"RDMA/NIXL transfer failed\": {\n",
    "        \"likely_cause\": \"InfiniBand misconfiguration\",\n",
    "        \"debug_commands\": [\n",
    "            \"ibstat  # Check IB port state\",\n",
    "            \"ibv_devinfo  # List RDMA devices\",\n",
    "            \"ib_write_bw  # Test RDMA bandwidth\"\n",
    "        ],\n",
    "        \"fix\": \"Verify IB cables, check subnet manager, ensure GPUDirect is enabled\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Dynamo Troubleshooting Quick Reference\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for symptom, details in troubleshooting_guide.items():\n",
    "    print(f\"\\nğŸ”´ Symptom: {symptom}\")\n",
    "    print(f\"   Likely cause: {details['likely_cause']}\")\n",
    "    print(f\"   Debug commands:\")\n",
    "    for cmd in details['debug_commands']:\n",
    "        print(f\"      $ {cmd}\")\n",
    "    print(f\"   Fix: {details['fix']}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "Complete these exercises to reinforce your understanding:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Calculate KV Cache Requirements\n",
    "\n",
    "Your DGX Spark has 96GB GPU memory. You want to serve **Llama-3-70B** with up to **16K context length**.\n",
    "\n",
    "1. How much memory does the model weights take (FP16)?\n",
    "2. How much memory does a single 16K context KV cache take?\n",
    "3. How many concurrent requests can you handle?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your solution here\n",
    "\n",
    "# Hint: Llama-3-70B has 70 billion parameters\n",
    "# FP16 = 2 bytes per parameter\n",
    "# Use the KV cache formula from earlier\n",
    "\n",
    "model_params_billions = 70\n",
    "bytes_per_param = 2  # FP16\n",
    "gpu_memory_gb = 96\n",
    "\n",
    "# Model weights size\n",
    "model_size_gb = ???  # TODO: Calculate\n",
    "\n",
    "# KV cache per request (use calculate_kv_cache_size from earlier)\n",
    "# Llama-3-70B: 80 layers, 8 KV heads, 128 head_dim\n",
    "kv_cache_per_request_gb = ???  # TODO: Calculate\n",
    "\n",
    "# Available memory for KV cache\n",
    "available_for_kv_gb = ???  # TODO: Calculate\n",
    "\n",
    "# Max concurrent requests\n",
    "max_concurrent = ???  # TODO: Calculate\n",
    "\n",
    "print(f\"Model weights: {model_size_gb:.1f} GB\")\n",
    "print(f\"KV cache per 16K request: {kv_cache_per_request_gb:.1f} GB\")\n",
    "print(f\"Available for KV cache: {available_for_kv_gb:.1f} GB\")\n",
    "print(f\"Max concurrent 16K requests: {max_concurrent}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Design a Routing Strategy\n",
    "\n",
    "You have 4 workers. Incoming request has prefix hash `abc123`. The KV Block Manager reports:\n",
    "\n",
    "| Worker | Has Prefix `abc123`? | Queue Depth | GPU Util |\n",
    "|--------|---------------------|-------------|----------|\n",
    "| W1     | Yes (80% match)     | 5           | 85%      |\n",
    "| W2     | No                  | 2           | 40%      |\n",
    "| W3     | Yes (40% match)     | 8           | 90%      |\n",
    "| W4     | No                  | 1           | 20%      |\n",
    "\n",
    "Which worker should the router select? Consider:\n",
    "1. Pure random routing - what would happen?\n",
    "2. Least-loaded routing - which worker?\n",
    "3. KV-aware routing - which worker and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your analysis here\n",
    "\n",
    "workers = {\n",
    "    \"W1\": {\"cache_hit\": 0.80, \"queue_depth\": 5, \"gpu_util\": 0.85},\n",
    "    \"W2\": {\"cache_hit\": 0.00, \"queue_depth\": 2, \"gpu_util\": 0.40},\n",
    "    \"W3\": {\"cache_hit\": 0.40, \"queue_depth\": 8, \"gpu_util\": 0.90},\n",
    "    \"W4\": {\"cache_hit\": 0.00, \"queue_depth\": 1, \"gpu_util\": 0.20},\n",
    "}\n",
    "\n",
    "# TODO: Implement your routing decision\n",
    "# Consider: cache hit rate saves compute, but queue depth adds latency\n",
    "\n",
    "def route_request(workers: dict) -> str:\n",
    "    \"\"\"\n",
    "    Return the worker ID that should handle this request.\n",
    "    \n",
    "    Hint: Balance cache hit benefit vs queue wait time.\n",
    "    If 80% cache hit saves 800 tokens of prefill (400ms),\n",
    "    is it worth waiting in a queue of 5?\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "selected = route_request(workers)\n",
    "print(f\"Selected worker: {selected}\")\n",
    "print(f\"Reasoning: ???\")  # TODO: Explain your choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Trace a Multi-Turn Conversation\n",
    "\n",
    "A user has a 3-turn conversation:\n",
    "\n",
    "1. \"What is Python?\"\n",
    "2. \"Show me an example\"\n",
    "3. \"Now in JavaScript\"\n",
    "\n",
    "For each turn, trace:\n",
    "- Which component handles it\n",
    "- Whether KV cache is reused\n",
    "- What events are published to NATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Your trace here\n",
    "\n",
    "conversation = [\n",
    "    {\"turn\": 1, \"user\": \"What is Python?\"},\n",
    "    {\"turn\": 2, \"user\": \"Show me an example\"},\n",
    "    {\"turn\": 3, \"user\": \"Now in JavaScript\"},\n",
    "]\n",
    "\n",
    "# TODO: For each turn, document:\n",
    "# - Cumulative prompt (includes previous turns)\n",
    "# - Prefix hash (would be computed by router)\n",
    "# - KV cache hit/miss\n",
    "# - Worker selected\n",
    "# - NATS events published\n",
    "\n",
    "for turn in conversation:\n",
    "    print(f\"\\nTurn {turn['turn']}: \\\"{turn['user']}\\\"\")\n",
    "    print(f\"  Cumulative prompt: ???\")\n",
    "    print(f\"  KV cache status: ???\") \n",
    "    print(f\"  Routed to: ???\")\n",
    "    print(f\"  NATS events: ???\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Dynamo is distributed systems for LLM inference** - All patterns you know (service discovery, load balancing, caching, message queues) apply here.\n",
    "\n",
    "2. **Two-phase inference is the key insight** - Prefill is compute-bound, decode is memory-bound. Disaggregating them enables specialization.\n",
    "\n",
    "3. **KV cache is the new session state** - Routing to maximize cache hits is like session affinity but for computed attention states.\n",
    "\n",
    "4. **etcd for \"who exists\", NATS for \"what's happening\"** - Service discovery vs event streaming serve different purposes.\n",
    "\n",
    "5. **RDMA matters at scale** - When transferring gigabytes of KV cache between GPUs, kernel bypass is essential.\n",
    "\n",
    "6. **Everything is observable** - Debug with etcdctl, nats-cli, nvidia-smi before diving into code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Now that you understand the architecture conceptually, proceed to:\n",
    "\n",
    "**[Module 01: Setup and First Inference](01-setup-and-first-inference.ipynb)**\n",
    "\n",
    "In the next module, we'll:\n",
    "1. Install Dynamo on your DGX Spark\n",
    "2. Start etcd and NATS manually\n",
    "3. Launch a single worker with vLLM\n",
    "4. Send your first inference request\n",
    "5. Trace the request through the system with logs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
