{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Module 02: Disaggregated Prefill-Decode Serving\n",
        "\n",
        "> **Goal**: Run prefill on GPU 0 and decode on GPU 1, observe KV cache transfer via NIXL/RDMA.\n",
        "\n",
        "---\n",
        "\n",
        "## What is Disaggregated Serving?\n",
        "\n",
        "LLM inference has two phases with very different characteristics:\n",
        "\n",
        "| Phase | What it does | Bottleneck | GPU Utilization |\n",
        "|-------|--------------|------------|------------------|\n",
        "| **Prefill** | Process entire prompt, generate KV cache | Compute-bound | High (matrix multiplications) |\n",
        "| **Decode** | Generate tokens one-by-one using KV cache | Memory-bound | Low (memory bandwidth limited) |\n",
        "\n",
        "**The Problem**: Running both phases on the same GPU leads to inefficiency:\n",
        "- During prefill: GPU compute is saturated, decode requests wait\n",
        "- During decode: GPU compute is underutilized, prefill requests wait\n",
        "\n",
        "**The Solution**: Disaggregated serving separates these phases onto different workers:\n",
        "\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ  Prefill Worker ‚îÇ     ‚îÇ  Decode Worker   ‚îÇ\n",
        "‚îÇ     (GPU 0)     ‚îÇ     ‚îÇ     (GPU 1)      ‚îÇ\n",
        "‚îÇ                 ‚îÇ     ‚îÇ                  ‚îÇ\n",
        "‚îÇ ‚Ä¢ Process prompt‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ ‚Ä¢ Generate tokens‚îÇ\n",
        "‚îÇ ‚Ä¢ Generate KV   ‚îÇ KV  ‚îÇ ‚Ä¢ Use KV cache   ‚îÇ\n",
        "‚îÇ                 ‚îÇcache‚îÇ                  ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "### The Bootstrap Server\n",
        "\n",
        "Each worker runs a **bootstrap server** - an HTTP endpoint that coordinates the KV cache transfer handshake:\n",
        "\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ  Prefill Worker ‚îÇ                    ‚îÇ  Decode Worker  ‚îÇ\n",
        "‚îÇ     (GPU 0)     ‚îÇ                    ‚îÇ     (GPU 1)     ‚îÇ\n",
        "‚îÇ                 ‚îÇ                    ‚îÇ                 ‚îÇ\n",
        "‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ  1. GET /route     ‚îÇ                 ‚îÇ\n",
        "‚îÇ ‚îÇ Bootstrap   ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ                 ‚îÇ\n",
        "‚îÇ ‚îÇ Server      ‚îÇ ‚îÇ  \"Where should I   ‚îÇ                 ‚îÇ\n",
        "‚îÇ ‚îÇ (HTTP)      ‚îÇ ‚îÇ   fetch KV from?\"  ‚îÇ                 ‚îÇ\n",
        "‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ                    ‚îÇ                 ‚îÇ\n",
        "‚îÇ                 ‚îú‚îÄ‚îÄ2. Route info‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ                 ‚îÇ\n",
        "‚îÇ                 ‚îÇ  (memory addrs,    ‚îÇ                 ‚îÇ\n",
        "‚îÇ                 ‚îÇ   rank info)       ‚îÇ                 ‚îÇ\n",
        "‚îÇ                 ‚îÇ                    ‚îÇ                 ‚îÇ\n",
        "‚îÇ  KV Cache ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ3. RDMA/NIXL‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  KV Cache       ‚îÇ\n",
        "‚îÇ  (GPU memory)   ‚îÇ  Direct Transfer   ‚îÇ  (GPU memory)   ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "**The handshake flow:**\n",
        "1. Decode worker receives a request needing KV cache from prefill\n",
        "2. Decode queries prefill's bootstrap server (`/route`) to get memory addresses and rank info\n",
        "3. NIXL performs direct GPU-to-GPU transfer via RDMA (fast, bypasses CPU)\n",
        "\n",
        "The bootstrap server handles **coordination metadata**, not actual data. The KV cache transfer uses RDMA for speed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 1: Verify Two GPUs Available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "GPU CHECK FOR DISAGGREGATED SERVING\n",
            "============================================================\n",
            "\n",
            "Found 2 GPU(s):\n",
            "\n",
            "  GPU 0: NVIDIA GeForce RTX 5090\n",
            "          Memory: 32109 MiB free / 32607 MiB total\n",
            "\n",
            "  GPU 1: NVIDIA GeForce RTX 5090\n",
            "          Memory: 32109 MiB free / 32607 MiB total\n",
            "\n",
            "‚úì Two GPUs available - ready for disaggregated serving!\n",
            "  ‚Ä¢ GPU 0 ‚Üí Prefill Worker\n",
            "  ‚Ä¢ GPU 1 ‚Üí Decode Worker\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"GPU CHECK FOR DISAGGREGATED SERVING\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "result = subprocess.run(\n",
        "    ['nvidia-smi', '--query-gpu=index,name,memory.total,memory.free', '--format=csv,noheader'],\n",
        "    capture_output=True, text=True, timeout=5\n",
        ")\n",
        "\n",
        "gpus = result.stdout.strip().split('\\n')\n",
        "print(f\"\\nFound {len(gpus)} GPU(s):\\n\")\n",
        "\n",
        "for gpu in gpus:\n",
        "    parts = gpu.split(', ')\n",
        "    idx, name, total, free = parts\n",
        "    print(f\"  GPU {idx}: {name}\")\n",
        "    print(f\"          Memory: {free} free / {total} total\")\n",
        "    print()\n",
        "\n",
        "if len(gpus) >= 2:\n",
        "    print(\"‚úì Two GPUs available - ready for disaggregated serving!\")\n",
        "    print(\"  ‚Ä¢ GPU 0 ‚Üí Prefill Worker\")\n",
        "    print(\"  ‚Ä¢ GPU 1 ‚Üí Decode Worker\")\n",
        "else:\n",
        "    print(\"‚úó Need at least 2 GPUs for disaggregated serving\")\n",
        "    print(\"  This notebook requires 2 GPUs on the same node.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 2: Verify Infrastructure (etcd)\n",
        "\n",
        "Make sure etcd is running from Module 01."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "INFRASTRUCTURE CHECK\n",
            "============================================================\n",
            "‚úì etcd: OK\n"
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "import json\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"INFRASTRUCTURE CHECK\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "try:\n",
        "    with urllib.request.urlopen(\"http://localhost:2379/health\", timeout=5) as resp:\n",
        "        print(\"‚úì etcd: OK\")\n",
        "except Exception as e:\n",
        "    print(f\"‚úó etcd: Not responding - {e}\")\n",
        "    print(\"\\n‚ö†Ô∏è  Start infrastructure first:\")\n",
        "    print(\"    Run Module 01 Step 3, or:\")\n",
        "    print(\"    docker start dynamo-etcd\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 3: Stop Any Existing Dynamo Processes\n",
        "\n",
        "Clean up from previous sessions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Finding Dynamo processes ===\n",
            "No Dynamo processes found\n",
            "\n",
            "=== Finding processes on port 8000 ===\n",
            "No processes on port 8000\n",
            "\n",
            "=== Sending SIGTERM to Dynamo processes ===\n",
            "No Dynamo processes to kill\n",
            "\n",
            "=== Sending SIGKILL to any remaining Dynamo processes ===\n",
            "No remaining processes\n",
            "\n",
            "=== Force killing processes on port 8000 ===\n",
            "No processes on port 8000 to kill\n",
            "\n",
            "=== Verifying cleanup ===\n",
            "‚úì All Dynamo processes stopped\n",
            "‚úì Port 8000 is free\n",
            "\n",
            "‚úì Cleanup complete\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "echo \"=== Finding Dynamo processes ===\"\n",
        "pgrep -af 'python -m dynamo' || echo \"No Dynamo processes found\"\n",
        "\n",
        "echo -e \"\\n=== Finding processes on port 8000 ===\"\n",
        "fuser -v 8000/tcp 2>&1 || echo \"No processes on port 8000\"\n",
        "\n",
        "echo -e \"\\n=== Sending SIGTERM to Dynamo processes ===\"\n",
        "pkill -f 'python -m dynamo' && echo \"Sent SIGTERM to Dynamo processes\" || echo \"No Dynamo processes to kill\"\n",
        "sleep 2\n",
        "\n",
        "echo -e \"\\n=== Sending SIGKILL to any remaining Dynamo processes ===\"\n",
        "pkill -9 -f 'python -m dynamo' && echo \"Sent SIGKILL to remaining processes\" || echo \"No remaining processes\"\n",
        "\n",
        "echo -e \"\\n=== Force killing processes on port 8000 ===\"\n",
        "fuser -k -9 8000/tcp 2>&1 && echo \"Killed processes on port 8000\" || echo \"No processes on port 8000 to kill\"\n",
        "\n",
        "sleep 3\n",
        "\n",
        "echo -e \"\\n=== Verifying cleanup ===\"\n",
        "pgrep -af 'python -m dynamo' && echo \"WARNING: Some processes still running!\" || echo \"‚úì All Dynamo processes stopped\"\n",
        "fuser -v 8000/tcp 2>&1 && echo \"WARNING: Port 8000 still in use!\" || echo \"‚úì Port 8000 is free\"\n",
        "echo -e \"\\n‚úì Cleanup complete\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 4: Install KV Cache Transfer Backend\n",
        "\n",
        "Disaggregated serving requires a backend to transfer KV cache between GPUs.\n",
        "\n",
        "| Backend | Description |\n",
        "|---------|-------------|\n",
        "| **NIXL** | NVIDIA's native transfer library for Dynamo (recommended) |\n",
        "| **Mooncake** | Third-party option |\n",
        "\n",
        "Both work on single-node setups. We'll use **NIXL** since it's designed for Dynamo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2mUsing Python 3.12.3 environment at: /root/src/github.com/sara4dev/ai-dynamo-the-hard-way/.venv\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 22ms\u001b[0m\u001b[0m\n",
            "\n",
            "‚úì NIXL (NVIDIA Inference Xfer Library) installed\n"
          ]
        }
      ],
      "source": [
        "!uv pip install nixl\n",
        "print(\"\\n‚úì NIXL (NVIDIA Inference Xfer Library) installed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 5: Launch Disaggregated Workers\n",
        "\n",
        "We'll start three processes:\n",
        "1. **Frontend** - HTTP API endpoint\n",
        "2. **Decode Worker** (GPU 1) - Generates tokens using KV cache\n",
        "3. **Prefill Worker** (GPU 0) - Processes prompts, generates KV cache\n",
        "\n",
        "Key flags:\n",
        "- `--disaggregation-mode prefill` or `decode` - which role this worker plays\n",
        "- `--disaggregation-transfer-backend nixl` - use NIXL for KV cache transfer\n",
        "- `--host 0.0.0.0` - **required** so the bootstrap server binds to all interfaces (not just localhost)\n",
        "- `CUDA_VISIBLE_DEVICES` - assign specific GPU to each worker\n",
        "\n",
        "> ‚ö†Ô∏è **Important**: Without `--host 0.0.0.0`, the bootstrap server binds to `127.0.0.1` only, causing \"Connection refused\" errors when workers try to communicate via the external IP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "LAUNCHING DISAGGREGATED SERVING\n",
            "============================================================\n",
            "\n",
            "[1/3] Starting Frontend...\n",
            "      PID: 2334384\n",
            "\n",
            "[2/3] Starting Decode Worker (GPU 1)...\n",
            "      PID: 2334385\n",
            "\n",
            "[3/3] Starting Prefill Worker (GPU 0)...\n",
            "      PID: 2334386\n",
            "\n",
            "‚úì All processes started\n",
            "  Logs: /tmp/dynamo_frontend.log, /tmp/dynamo_prefill.log, /tmp/dynamo_decode.log\n",
            "\n",
            "‚è≥ Wait ~60s for models to load on both GPUs...\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "MODEL=\"Qwen/Qwen3-0.6B\"\n",
        "\n",
        "echo \"============================================================\"\n",
        "echo \"LAUNCHING DISAGGREGATED SERVING\"\n",
        "echo \"============================================================\"\n",
        "\n",
        "# Start Frontend\n",
        "echo \"\"\n",
        "echo \"[1/3] Starting Frontend...\"\n",
        "python -m dynamo.frontend > /tmp/dynamo_frontend.log 2>&1 &\n",
        "echo \"      PID: $!\"\n",
        "\n",
        "# Start Decode Worker on GPU 1 (decode workers start first in Dynamo)\n",
        "# NOTE: --host 0.0.0.0 is required so the bootstrap server binds to all interfaces\n",
        "echo \"\"\n",
        "echo \"[2/3] Starting Decode Worker (GPU 1)...\"\n",
        "CUDA_VISIBLE_DEVICES=1 python -m dynamo.sglang \\\n",
        "    --model-path $MODEL \\\n",
        "    --host 0.0.0.0 \\\n",
        "    --attention-backend flashinfer \\\n",
        "    --disaggregation-mode decode \\\n",
        "    --disaggregation-transfer-backend nixl \\\n",
        "    > /tmp/dynamo_decode.log 2>&1 &\n",
        "echo \"      PID: $!\"\n",
        "\n",
        "# Start Prefill Worker on GPU 0\n",
        "# NOTE: --host 0.0.0.0 is required so the bootstrap server binds to all interfaces\n",
        "echo \"\"\n",
        "echo \"[3/3] Starting Prefill Worker (GPU 0)...\"\n",
        "CUDA_VISIBLE_DEVICES=0 python -m dynamo.sglang \\\n",
        "    --model-path $MODEL \\\n",
        "    --host 0.0.0.0 \\\n",
        "    --attention-backend flashinfer \\\n",
        "    --disaggregation-mode prefill \\\n",
        "    --disaggregation-transfer-backend nixl \\\n",
        "    > /tmp/dynamo_prefill.log 2>&1 &\n",
        "echo \"      PID: $!\"\n",
        "\n",
        "echo \"\"\n",
        "echo \"‚úì All processes started\"\n",
        "echo \"  Logs: /tmp/dynamo_frontend.log, /tmp/dynamo_prefill.log, /tmp/dynamo_decode.log\"\n",
        "echo \"\"\n",
        "echo \"‚è≥ Wait ~60s for models to load on both GPUs...\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 6: Wait for Workers to Register"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Waiting for disaggregated workers to start...\n",
            "(This may take ~60-90 seconds for both models to load)\n",
            "\n",
            "[0s] Waiting for frontend...\n",
            "[10s] Frontend: ‚úì | Workers registered: 0\n",
            "[20s] Frontend: ‚úì | Workers registered: 0\n",
            "[30s] Frontend: ‚úì | Workers registered: 0\n",
            "[40s] Frontend: ‚úì | Workers registered: 0\n",
            "[50s] Frontend: ‚úì | Workers registered: 0\n",
            "[60s] Frontend: ‚úì | Workers registered: 0\n",
            "[70s] Frontend: ‚úì | Workers registered: 0\n",
            "[80s] Frontend: ‚úì | Workers registered: 0\n",
            "[90s] Frontend: ‚úì | Workers registered: 2\n",
            "\n",
            "‚úì Disaggregated setup ready!\n",
            "\n",
            "Registered instances:\n",
            "  ‚Ä¢ backend/generate\n",
            "    Transport: {'tcp': '192.168.1.180:36463/18f9c1b0f95b2ca/generate'}\n",
            "  ‚Ä¢ prefill/generate\n",
            "    Transport: {'tcp': '192.168.1.180:36349/18f9c1b0f95b2c4/generate'}\n"
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "import json\n",
        "import time\n",
        "\n",
        "print(\"Waiting for disaggregated workers to start...\")\n",
        "print(\"(This may take ~60-90 seconds for both models to load)\\n\")\n",
        "\n",
        "MAX_WAIT = 180\n",
        "INTERVAL = 10\n",
        "elapsed = 0\n",
        "\n",
        "while elapsed < MAX_WAIT:\n",
        "    try:\n",
        "        with urllib.request.urlopen('http://localhost:8000/health', timeout=5) as response:\n",
        "            health = json.loads(response.read())\n",
        "            instances = health.get('instances', [])\n",
        "            \n",
        "            # Count worker types\n",
        "            prefill_workers = [i for i in instances if 'prefill' in str(i).lower()]\n",
        "            decode_workers = [i for i in instances if 'decode' in str(i).lower() or 'backend' in str(i.get('component', '')).lower()]\n",
        "            \n",
        "            print(f\"[{elapsed}s] Frontend: ‚úì | Workers registered: {len(instances)}\")\n",
        "            \n",
        "            # For disaggregated, we need at least 2 workers\n",
        "            if len(instances) >= 2:\n",
        "                print(f\"\\n‚úì Disaggregated setup ready!\")\n",
        "                print(f\"\\nRegistered instances:\")\n",
        "                for inst in instances:\n",
        "                    print(f\"  ‚Ä¢ {inst.get('component', 'unknown')}/{inst.get('endpoint', 'unknown')}\")\n",
        "                    print(f\"    Transport: {inst.get('transport', {})}\")\n",
        "                break\n",
        "                \n",
        "    except Exception as e:\n",
        "        print(f\"[{elapsed}s] Waiting for frontend...\")\n",
        "    \n",
        "    time.sleep(INTERVAL)\n",
        "    elapsed += INTERVAL\n",
        "\n",
        "if elapsed >= MAX_WAIT:\n",
        "    print(f\"\\n‚ö†Ô∏è  Timeout after {MAX_WAIT}s\")\n",
        "    print(\"\\nCheck logs for errors:\")\n",
        "    print(\"  tail -50 /tmp/dynamo_prefill.log\")\n",
        "    print(\"  tail -50 /tmp/dynamo_decode.log\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 7: Check etcd Registrations\n",
        "\n",
        "Let's see how both workers registered themselves in etcd."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "ETCD WORKER REGISTRATIONS\n",
            "============================================================\n",
            "\n",
            "üü¢ DECODE: v1/instances/dynamo/backend/generate/18f9c1b0f95b2ca\n",
            "   Type: Endpoint\n",
            "   Transport: {'tcp': '192.168.1.180:36463/18f9c1b0f95b2ca/generate'}\n",
            "\n",
            "üîµ PREFILL: v1/instances/dynamo/prefill/generate/18f9c1b0f95b2c4\n",
            "   Type: Endpoint\n",
            "   Transport: {'tcp': '192.168.1.180:36349/18f9c1b0f95b2c4/generate'}\n",
            "\n",
            "üü¢ DECODE: v1/mdc/dynamo/backend/generate/18f9c1b0f95b2ca\n",
            "   Type: Model\n",
            "\n",
            "üîµ PREFILL: v1/mdc/dynamo/prefill/generate/18f9c1b0f95b2c4\n",
            "   Type: Model\n"
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "import json\n",
        "import base64\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"ETCD WORKER REGISTRATIONS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "try:\n",
        "    req = urllib.request.Request(\n",
        "        \"http://localhost:2379/v3/kv/range\",\n",
        "        data=json.dumps({\n",
        "            \"key\": base64.b64encode(b\"v1/\").decode(),\n",
        "            \"range_end\": base64.b64encode(b\"v10\").decode()\n",
        "        }).encode(),\n",
        "        headers={'Content-Type': 'application/json'}\n",
        "    )\n",
        "    with urllib.request.urlopen(req, timeout=5) as resp:\n",
        "        data = json.loads(resp.read())\n",
        "        \n",
        "        if 'kvs' in data and data['kvs']:\n",
        "            for kv in data['kvs']:\n",
        "                key = base64.b64decode(kv['key']).decode()\n",
        "                value = json.loads(base64.b64decode(kv['value']).decode())\n",
        "                \n",
        "                # Highlight prefill vs decode\n",
        "                if 'prefill' in key.lower():\n",
        "                    print(f\"\\nüîµ PREFILL: {key}\")\n",
        "                elif 'decode' in key.lower() or 'backend' in key.lower():\n",
        "                    print(f\"\\nüü¢ DECODE: {key}\")\n",
        "                else:\n",
        "                    print(f\"\\n‚ö™ {key}\")\n",
        "                    \n",
        "                print(f\"   Type: {value.get('type', 'unknown')}\")\n",
        "                if 'transport' in value:\n",
        "                    print(f\"   Transport: {value['transport']}\")\n",
        "        else:\n",
        "            print(\"No workers registered yet\")\n",
        "            \n",
        "except Exception as e:\n",
        "    print(f\"Error querying etcd: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 8: Test Disaggregated Inference\n",
        "\n",
        "Let's send a request and verify the disaggregated setup is working."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "TESTING DISAGGREGATED INFERENCE\n",
            "============================================================\n",
            "\n",
            "‚úì Inference complete!\n",
            "\n",
            "Response:\n",
            "<think>\n",
            "Okay, the user wants me to explain quantum computing in two sentences. Let me start with the basics. I know quantum computing uses qubits instead of classical bits. Qubits are fundamental because they can be in a superposition state, right? That's a key point. So first sentence: \"Quantum computing uses qubits, which are quantum mechanical particles, to enable computing that can process multiple possibilities at once.\"\n",
            "\n",
            "Now the second sentence. I need to mention the interference and entang\n"
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "import json\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"TESTING DISAGGREGATED INFERENCE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "payload = {\n",
        "    \"model\": \"Qwen/Qwen3-0.6B\",\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"Explain quantum computing in 2 sentences.\"}],\n",
        "    \"max_tokens\": 100\n",
        "}\n",
        "\n",
        "try:\n",
        "    req = urllib.request.Request(\n",
        "        \"http://localhost:8000/v1/chat/completions\",\n",
        "        data=json.dumps(payload).encode(),\n",
        "        headers={'Content-Type': 'application/json'}\n",
        "    )\n",
        "    with urllib.request.urlopen(req, timeout=120) as resp:\n",
        "        result = json.loads(resp.read())\n",
        "        content = result['choices'][0]['message']['content']\n",
        "        print(f\"\\n‚úì Inference complete!\")\n",
        "        print(f\"\\nResponse:\\n{content}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚úó Inference failed: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Prefill Worker Log (last 5 lines) ===\n",
            "\u001b[2m2026-02-01T23:25:14.782379Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mdynamo_llm::hub\u001b[0m\u001b[2m:\u001b[0m ModelExpress download completed successfully for model: Qwen/Qwen3-0.6B\n",
            "\u001b[2m2026-02-01T23:25:14.807789Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2m_core\u001b[0m\u001b[2m:\u001b[0m Registered base model 'Qwen/Qwen3-0.6B' MDC\n",
            "\u001b[2m2026-02-01T23:25:14.808546Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mregister._register_llm_with_runtime_config\u001b[0m\u001b[2m:\u001b[0m Successfully registered LLM with runtime config\n",
            "\u001b[2m2026-02-01T23:25:14.808844Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mregister.register_llm_with_readiness_gate\u001b[0m\u001b[2m:\u001b[0m Model registration succeeded; processing queued requests\n",
            "\u001b[2m2026-02-01T23:25:23.957274Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mscheduler_metrics_mixin.log_prefill_stats\u001b[0m\u001b[2m:\u001b[0m Prefill batch, #new-seq: 1, #new-token: 17, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, #prealloc-req: 0, #inflight-req: 0, input throughput (token/s): 0.00, \n",
            "\n",
            "=== Decode Worker Log (last 5 lines) ===\n",
            "\u001b[2m2026-02-01T23:25:20.896835Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2m_core\u001b[0m\u001b[2m:\u001b[0m Registered base model 'Qwen/Qwen3-0.6B' MDC\n",
            "\u001b[2m2026-02-01T23:25:20.897781Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mregister._register_llm_with_runtime_config\u001b[0m\u001b[2m:\u001b[0m Successfully registered LLM with runtime config\n",
            "\u001b[2m2026-02-01T23:25:20.898113Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mregister.register_llm_with_readiness_gate\u001b[0m\u001b[2m:\u001b[0m Model registration succeeded; processing queued requests\n",
            "\u001b[2m2026-02-01T23:25:32.494351Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mscheduler_metrics_mixin.log_decode_stats\u001b[0m\u001b[2m:\u001b[0m Decode batch, #running-req: 1, #token: 58, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 3.19, #queue-req: 0, \n",
            "\u001b[2m2026-02-01T23:25:32.766407Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mscheduler_metrics_mixin.log_decode_stats\u001b[0m\u001b[2m:\u001b[0m Decode batch, #running-req: 1, #token: 98, token usage: 0.00, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 146.99, #queue-req: 0, \n"
          ]
        }
      ],
      "source": [
        "# Check logs to verify disaggregation is working\n",
        "!echo \"=== Prefill Worker Log (last 5 lines) ===\" && tail -5 /tmp/dynamo_prefill.log | grep -E \"(Prefill|INFO)\" || echo \"No prefill activity\"\n",
        "!echo -e \"\\n=== Decode Worker Log (last 5 lines) ===\" && tail -5 /tmp/dynamo_decode.log | grep -E \"(Decode|INFO)\" || echo \"No decode activity\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 9: Verify GPU Utilization\n",
        "\n",
        "Let's confirm both GPUs are being used during inference by monitoring utilization while sending requests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "GPU UTILIZATION DURING INFERENCE\n",
            "============================================================\n",
            "\n",
            "GPU 0 (Prefill): avg 0.0% | max 0%\n",
            "GPU 1 (Decode):  avg 0.8% | max 20%\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import time\n",
        "import urllib.request\n",
        "import json\n",
        "import threading\n",
        "\n",
        "def monitor_gpus(duration=10):\n",
        "    \"\"\"Monitor GPU utilization for a duration\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"GPU UTILIZATION DURING INFERENCE\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    samples = []\n",
        "    start = time.time()\n",
        "    \n",
        "    while time.time() - start < duration:\n",
        "        result = subprocess.run(\n",
        "            ['nvidia-smi', '--query-gpu=index,utilization.gpu,memory.used', '--format=csv,noheader,nounits'],\n",
        "            capture_output=True, text=True\n",
        "        )\n",
        "        samples.append(result.stdout.strip())\n",
        "        time.sleep(0.5)\n",
        "    \n",
        "    # Parse and summarize\n",
        "    gpu0_util = []\n",
        "    gpu1_util = []\n",
        "    \n",
        "    for sample in samples:\n",
        "        for line in sample.split('\\n'):\n",
        "            parts = line.split(', ')\n",
        "            if len(parts) >= 2:\n",
        "                idx, util = int(parts[0]), int(parts[1])\n",
        "                if idx == 0:\n",
        "                    gpu0_util.append(util)\n",
        "                elif idx == 1:\n",
        "                    gpu1_util.append(util)\n",
        "    \n",
        "    print(f\"\\nGPU 0 (Prefill): avg {sum(gpu0_util)/len(gpu0_util):.1f}% | max {max(gpu0_util)}%\")\n",
        "    print(f\"GPU 1 (Decode):  avg {sum(gpu1_util)/len(gpu1_util):.1f}% | max {max(gpu1_util)}%\")\n",
        "\n",
        "def send_inference_request():\n",
        "    time.sleep(1)\n",
        "    payload = {\n",
        "        \"model\": \"Qwen/Qwen3-0.6B\",\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": \"Write a haiku about computers.\"}],\n",
        "        \"max_tokens\": 50\n",
        "    }\n",
        "    try:\n",
        "        req = urllib.request.Request(\n",
        "            \"http://localhost:8000/v1/chat/completions\",\n",
        "            data=json.dumps(payload).encode(),\n",
        "            headers={'Content-Type': 'application/json'}\n",
        "        )\n",
        "        urllib.request.urlopen(req, timeout=60)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "# Start inference in background and monitor GPUs\n",
        "thread = threading.Thread(target=send_inference_request)\n",
        "thread.start()\n",
        "monitor_gpus(duration=15)\n",
        "thread.join()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Understanding the Request Flow\n",
        "\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ  Client  ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Frontend ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Decode Worker‚îÇ‚óÄ‚îÄ‚îÄ‚ñ∂‚îÇPrefill Worker‚îÇ\n",
        "‚îÇ          ‚îÇ    ‚îÇ  (:8000) ‚îÇ    ‚îÇ   (GPU 1)    ‚îÇ    ‚îÇ   (GPU 0)    ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                     ‚îÇ                 ‚îÇ                   ‚îÇ\n",
        "                     ‚îÇ                 ‚îÇ    KV Cache       ‚îÇ\n",
        "                     ‚ñº                 ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄTransfer‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ\n",
        "                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê             ‚îÇ   (RDMA/NIXL)     ‚îÇ\n",
        "                  ‚îÇ etcd ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         Worker Discovery\n",
        "```\n",
        "\n",
        "### Detailed Flow:\n",
        "\n",
        "1. **Client ‚Üí Frontend**: HTTP request arrives\n",
        "2. **Frontend ‚Üí Decode Worker**: Routes to decode worker (discovered via etcd)\n",
        "3. **Decode ‚Üí Prefill**: Decode forwards prompt to prefill for KV cache generation\n",
        "4. **Prefill processing**: Processes prompt, generates KV cache on GPU 0\n",
        "5. **KV Transfer**: KV cache transferred via RDMA directly to GPU 1 memory\n",
        "6. **Decode generates**: Decode worker generates tokens using transferred KV cache\n",
        "7. **Response**: Tokens stream back to client\n",
        "\n",
        "### Key Components:\n",
        "\n",
        "- **etcd**: Service discovery - workers register themselves, frontend finds them\n",
        "- **Bootstrap Server**: HTTP endpoint on each worker for KV transfer handshake\n",
        "- **NIXL/RDMA**: Direct GPU-to-GPU memory transfer (fast, bypasses CPU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Key Takeaways\n",
        "\n",
        "1. **Disaggregated serving** separates compute-bound prefill from memory-bound decode\n",
        "2. **etcd** is used for worker discovery - both workers register independently\n",
        "3. **Bootstrap server** on each worker handles KV transfer coordination via HTTP\n",
        "4. **NIXL/RDMA** transfers KV cache directly between GPU memories (fast!)\n",
        "5. **Both GPUs active** - each specialized for its phase\n",
        "\n",
        "### Performance Benefits\n",
        "\n",
        "| Metric | Single Worker | Disaggregated |\n",
        "|--------|---------------|---------------|\n",
        "| Prefill throughput | Limited by decode | Dedicated GPU |\n",
        "| Decode latency | Interrupted by prefill | Uninterrupted |\n",
        "| GPU utilization | Mixed workload | Optimized per-phase |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Cleanup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stopping Dynamo processes...\n",
            "‚úì Cleanup complete\n",
            "\\nGPU memory after cleanup:\n",
            "index, memory.used [MiB], memory.free [MiB]\n",
            "0, 2 MiB, 32109 MiB\n",
            "1, 2 MiB, 32109 MiB\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "echo \"Stopping Dynamo processes...\"\n",
        "pkill -f 'python -m dynamo' 2>/dev/null || true\n",
        "echo \"‚úì Cleanup complete\"\n",
        "\n",
        "# Show GPU memory freed\n",
        "sleep 2\n",
        "echo \"\\nGPU memory after cleanup:\"\n",
        "nvidia-smi --query-gpu=index,memory.used,memory.free --format=csv"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
